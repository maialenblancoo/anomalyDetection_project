{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZUT3EtJX8SF"
      },
      "outputs": [],
      "source": [
        "# **Transfer Learning for Time Series Anomaly Detection**\n",
        "# \n",
        "# This notebook implements transfer learning approaches to enhance anomaly detection\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import logging\n",
        "import os\n",
        "\n",
        "# Import custom modules\n",
        "import sys\n",
        "sys.path.append('./')\n",
        "from utils.data_loader import load_data, normalize_data, create_sequences\n",
        "from utils.model_utils import LSTMAutoencoder, GRUAutoencoder, TransformerEncoder, train_model\n",
        "from utils.model_utils import get_reconstruction_errors, EnsembleModel\n",
        "from utils.evaluation import evaluate_threshold, AnomalyInterpreter, visualize_results\n",
        "from utils.evaluation import plot_roc_curves, plot_precision_recall_curves\n",
        "\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_context(\"notebook\", font_scale=1.2)\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "logger.info(f\"Usando dispositivo: {device}\")\n",
        "\n",
        "# Load pre-trained models\n",
        "def load_pretrained_models(base_path=\"models/autoencoder/\"):\n",
        "    \"\"\"Load pre-trained autoencoder models.\"\"\"\n",
        "    # Load model metadata\n",
        "    import json\n",
        "    \n",
        "    with open(os.path.join(base_path, \"model_metadata.json\"), \"r\") as f:\n",
        "        metadata = json.load(f)\n",
        "    \n",
        "    seq_len = metadata[\"sequence_length\"]\n",
        "    embedding_dim = metadata[\"embedding_dim\"]\n",
        "    \n",
        "    # Initialize models\n",
        "    lstm_model = LSTMAutoencoder(seq_len=seq_len, n_features=1, embedding_dim=embedding_dim)\n",
        "    gru_model = GRUAutoencoder(seq_len=seq_len, n_features=1, embedding_dim=embedding_dim)\n",
        "    transformer_model = TransformerEncoder(seq_len=seq_len, n_features=1, d_model=embedding_dim)\n",
        "    \n",
        "    # Load weights\n",
        "    lstm_model.load_state_dict(torch.load(os.path.join(base_path, \"lstm_model.pt\")))\n",
        "    gru_model.load_state_dict(torch.load(os.path.join(base_path, \"gru_model.pt\")))\n",
        "    transformer_model.load_state_dict(torch.load(os.path.join(base_path, \"transformer_model.pt\")))\n",
        "    \n",
        "    return {\n",
        "        \"lstm\": lstm_model,\n",
        "        \"gru\": gru_model,\n",
        "        \"transformer\": transformer_model\n",
        "    }, metadata\n",
        "\n",
        "# Load new dataset\n",
        "def load_new_dataset(filepath, sequence_length=150, step=10, train_ratio=0.7, val_ratio=0.15):\n",
        "    \"\"\"Load and prepare a new dataset for transfer learning.\"\"\"\n",
        "    df = load_data(filepath)\n",
        "    data_scaled, scaler = normalize_data(df[['value']])\n",
        "    \n",
        "    X = create_sequences(data_scaled, seq_length=sequence_length, step=step)\n",
        "    \n",
        "    # Split data without labels\n",
        "    train_size = int(len(X) * train_ratio)\n",
        "    val_size = int(len(X) * val_ratio)\n",
        "    \n",
        "    X_train = X[:train_size]\n",
        "    X_val = X[train_size:train_size+val_size]\n",
        "    X_test = X[train_size+val_size:]\n",
        "    \n",
        "    # Create dataloaders\n",
        "    batch_size = 64\n",
        "    X_train_tensor = torch.FloatTensor(X_train)\n",
        "    X_val_tensor = torch.FloatTensor(X_val)\n",
        "    X_test_tensor = torch.FloatTensor(X_test)\n",
        "    \n",
        "    train_dataset = TensorDataset(X_train_tensor)\n",
        "    val_dataset = TensorDataset(X_val_tensor)\n",
        "    test_dataset = TensorDataset(X_test_tensor)\n",
        "    \n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "    \n",
        "    return {\n",
        "        \"dataframe\": df,\n",
        "        \"scaler\": scaler,\n",
        "        \"sequences\": {\n",
        "            \"X_train\": X_train,\n",
        "            \"X_val\": X_val,\n",
        "            \"X_test\": X_test\n",
        "        },\n",
        "        \"loaders\": {\n",
        "            \"train\": train_loader,\n",
        "            \"val\": val_loader,\n",
        "            \"test\": test_loader\n",
        "        }\n",
        "    }\n",
        "\n",
        "# Apply transfer learning\n",
        "def apply_transfer_learning(pretrained_model, new_data_loaders, freeze_encoder=True, n_epochs=20, learning_rate=1e-4):\n",
        "    \"\"\"Apply transfer learning to a pre-trained model on new data.\"\"\"\n",
        "    model = pretrained_model.to(device)\n",
        "    \n",
        "    # Freeze encoder layers if specified\n",
        "    if freeze_encoder:\n",
        "        for name, param in model.named_parameters():\n",
        "            if 'encoder' in name:\n",
        "                param.requires_grad = False\n",
        "    \n",
        "    # Fine-tune on new data\n",
        "    history, fine_tuned_model = train_model(\n",
        "        model, new_data_loaders[\"train\"], new_data_loaders[\"val\"],\n",
        "        n_epochs=n_epochs, learning_rate=learning_rate,\n",
        "        device=device, patience=5, min_delta=0.0001\n",
        "    )\n",
        "    \n",
        "    return history, fine_tuned_model\n",
        "\n",
        "# Load a new dataset\n",
        "new_filepath = 'data/ambient_temperature_system_failure.csv'  # Replace with your new dataset\n",
        "new_data = load_new_dataset(new_filepath)\n",
        "\n",
        "# Load pre-trained models\n",
        "pretrained_models, metadata = load_pretrained_models()\n",
        "\n",
        "# Apply transfer learning to LSTM model\n",
        "lstm_history, fine_tuned_lstm = apply_transfer_learning(\n",
        "    pretrained_models[\"lstm\"], new_data[\"loaders\"],\n",
        "    freeze_encoder=True, n_epochs=20\n",
        ")\n",
        "\n",
        "# Apply transfer learning to GRU model\n",
        "gru_history, fine_tuned_gru = apply_transfer_learning(\n",
        "    pretrained_models[\"gru\"], new_data[\"loaders\"],\n",
        "    freeze_encoder=True, n_epochs=20\n",
        ")\n",
        "\n",
        "# Apply transfer learning to Transformer model\n",
        "transformer_history, fine_tuned_transformer = apply_transfer_learning(\n",
        "    pretrained_models[\"transformer\"], new_data[\"loaders\"],\n",
        "    freeze_encoder=True, n_epochs=20\n",
        ")\n",
        "\n",
        "# Get reconstruction errors\n",
        "fine_tuned_models = {\n",
        "    \"lstm\": fine_tuned_lstm,\n",
        "    \"gru\": fine_tuned_gru,\n",
        "    \"transformer\": fine_tuned_transformer\n",
        "}\n",
        "\n",
        "# Get errors for validation data\n",
        "val_errors = {}\n",
        "for name, model in fine_tuned_models.items():\n",
        "    errors, _, _ = get_reconstruction_errors(model, new_data[\"loaders\"][\"val\"], device)\n",
        "    val_errors[name] = errors\n",
        "\n",
        "# Get errors for test data\n",
        "test_errors = {}\n",
        "test_reconstructions = {}\n",
        "for name, model in fine_tuned_models.items():\n",
        "    errors, orig_data, recon_data = get_reconstruction_errors(model, new_data[\"loaders\"][\"test\"], device)\n",
        "    test_errors[name] = errors\n",
        "    test_reconstructions[name] = {\n",
        "        \"original\": orig_data,\n",
        "        \"reconstructed\": recon_data\n",
        "    }\n",
        "\n",
        "# Create an ensemble model\n",
        "ensemble = EnsembleModel(\n",
        "    models=[fine_tuned_lstm, fine_tuned_gru, fine_tuned_transformer],\n",
        "    names=[\"LSTM\", \"GRU\", \"Transformer\"]\n",
        ")\n",
        "\n",
        "# Optimize ensemble weights\n",
        "val_ensemble_errors, _ = ensemble.get_weighted_errors(\n",
        "    [new_data[\"loaders\"][\"val\"]] * 3,\n",
        "    device\n",
        ")\n",
        "\n",
        "# Get ensemble errors for test data\n",
        "ensemble_errors, _ = ensemble.get_weighted_errors(\n",
        "    [new_data[\"loaders\"][\"test\"]] * 3,\n",
        "    device\n",
        ")\n",
        "\n",
        "# Set a threshold for anomaly detection\n",
        "def set_adaptive_threshold(errors, factor=2.0):\n",
        "    \"\"\"Set an adaptive threshold for anomaly detection.\"\"\"\n",
        "    threshold = np.mean(errors) + factor * np.std(errors)\n",
        "    return threshold\n",
        "\n",
        "# Set thresholds\n",
        "thresholds = {}\n",
        "for name, errors in val_errors.items():\n",
        "    thresholds[name] = set_adaptive_threshold(errors)\n",
        "\n",
        "ensemble_threshold = set_adaptive_threshold(val_ensemble_errors)\n",
        "\n",
        "# Detect anomalies\n",
        "anomalies = {}\n",
        "for name, errors in test_errors.items():\n",
        "    anomalies[name] = errors > thresholds[name]\n",
        "\n",
        "ensemble_anomalies = ensemble_errors > ensemble_threshold\n",
        "\n",
        "# Visualize anomalies\n",
        "def plot_anomalies(errors, threshold, anomalies, title):\n",
        "    plt.figure(figsize=(15, 6))\n",
        "    \n",
        "    plt.plot(errors, label='Reconstruction Error')\n",
        "    plt.axhline(y=threshold, color='r', linestyle='--', label=f'Threshold ({threshold:.6f})')\n",
        "    \n",
        "    # Mark anomalies\n",
        "    anomaly_indices = np.where(anomalies)[0]\n",
        "    plt.scatter(anomaly_indices, errors[anomaly_indices], color='red', label='Anomalies')\n",
        "    \n",
        "    plt.title(title)\n",
        "    plt.xlabel('Sample Index')\n",
        "    plt.ylabel('Reconstruction Error')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    \n",
        "    return plt.gcf()\n",
        "\n",
        "# Plot ensemble anomalies\n",
        "plot_anomalies(ensemble_errors, ensemble_threshold, ensemble_anomalies, 'Ensemble Anomaly Detection')\n",
        "plt.savefig('transfer_learning_anomalies.png', dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# Compare original model vs transfer learned model\n",
        "def compare_models(original_model, transfer_model, test_loader, device, title):\n",
        "    \"\"\"Compare performance of original vs transfer learned model.\"\"\"\n",
        "    original_errors, _, _ = get_reconstruction_errors(original_model, test_loader, device)\n",
        "    transfer_errors, _, _ = get_reconstruction_errors(transfer_model, test_loader, device)\n",
        "    \n",
        "    original_threshold = np.mean(original_errors) + 2.0 * np.std(original_errors)\n",
        "    transfer_threshold = np.mean(transfer_errors) + 2.0 * np.std(transfer_errors)\n",
        "    \n",
        "    plt.figure(figsize=(15, 8))\n",
        "    \n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.plot(original_errors, label='Original Model Errors')\n",
        "    plt.axhline(y=original_threshold, color='r', linestyle='--', label=f'Threshold ({original_threshold:.6f})')\n",
        "    plt.title(f'Original {title} Model')\n",
        "    plt.xlabel('Sample Index')\n",
        "    plt.ylabel('Reconstruction Error')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    \n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.plot(transfer_errors, label='Transfer Model Errors')\n",
        "    plt.axhline(y=transfer_threshold, color='r', linestyle='--', label=f'Threshold ({transfer_threshold:.6f})')\n",
        "    plt.title(f'Transfer Learned {title} Model')\n",
        "    plt.xlabel('Sample Index')\n",
        "    plt.ylabel('Reconstruction Error')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    return plt.gcf()\n",
        "\n",
        "# Compare LSTM original vs transfer learned\n",
        "compare_models(pretrained_models[\"lstm\"], fine_tuned_lstm, new_data[\"loaders\"][\"test\"], device, \"LSTM\")\n",
        "plt.savefig('lstm_transfer_comparison.png', dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# Save transfer learned models\n",
        "def save_transfer_models(models_dict, base_path=\"models/transfer/\"):\n",
        "    \"\"\"Save transfer learned models.\"\"\"\n",
        "    import os\n",
        "    import json\n",
        "    \n",
        "    os.makedirs(base_path, exist_ok=True)\n",
        "    \n",
        "    for name, model in models_dict.items():\n",
        "        model_path = os.path.join(base_path, f\"{name}_model.pt\")\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "        logger.info(f\"Modelo transfer learning {name} guardado en {model_path}\")\n",
        "    \n",
        "    # Save ensemble weights\n",
        "    weights_dict = {\n",
        "        \"lstm\": float(ensemble.weights[0]),\n",
        "        \"gru\": float(ensemble.weights[1]),\n",
        "        \"transformer\": float(ensemble.weights[2])\n",
        "    }\n",
        "    \n",
        "    with open(os.path.join(base_path, \"ensemble_weights.json\"), \"w\") as f:\n",
        "        json.dump(weights_dict, f)\n",
        "    \n",
        "    # Save thresholds\n",
        "    thresholds_dict = {\n",
        "        \"lstm\": float(thresholds[\"lstm\"]),\n",
        "        \"gru\": float(thresholds[\"gru\"]),\n",
        "        \"transformer\": float(thresholds[\"transformer\"]),\n",
        "        \"ensemble\": float(ensemble_threshold)\n",
        "    }\n",
        "    \n",
        "    with open(os.path.join(base_path, \"thresholds.json\"), \"w\") as f:\n",
        "        json.dump(thresholds_dict, f)\n",
        "    \n",
        "    # Save metadata\n",
        "    metadata = {\n",
        "        \"sequence_length\": metadata[\"sequence_length\"],\n",
        "        \"embedding_dim\": metadata[\"embedding_dim\"],\n",
        "        \"source_dataset\": \"machine_temperature_system_failure.csv\",\n",
        "        \"target_dataset\": new_filepath\n",
        "    }\n",
        "    \n",
        "    with open(os.path.join(base_path, \"transfer_metadata.json\"), \"w\") as f:\n",
        "        json.dump(metadata, f)\n",
        "    \n",
        "    logger.info(f\"Transfer learning models saved to {base_path}\")\n",
        "\n",
        "# Save transfer learned models\n",
        "save_transfer_models(\n",
        "    {\n",
        "        \"lstm\": fine_tuned_lstm,\n",
        "        \"gru\": fine_tuned_gru,\n",
        "        \"transformer\": fine_tuned_transformer\n",
        "    },\n",
        "    base_path=\"models/transfer/\"\n",
        ")\n",
        "\n",
        "# Function to load transfer learned models\n",
        "def load_transfer_models(base_path=\"models/transfer/\"):\n",
        "    \"\"\"Load transfer learned models.\"\"\"\n",
        "    import os\n",
        "    import json\n",
        "    \n",
        "    # Load metadata\n",
        "    with open(os.path.join(base_path, \"transfer_metadata.json\"), \"r\") as f:\n",
        "        metadata = json.load(f)\n",
        "    \n",
        "    seq_len = metadata[\"sequence_length\"]\n",
        "    embedding_dim = metadata[\"embedding_dim\"]\n",
        "    \n",
        "    # Initialize models\n",
        "    lstm_model = LSTMAutoencoder(seq_len=seq_len, n_features=1, embedding_dim=embedding_dim)\n",
        "    gru_model = GRUAutoencoder(seq_len=seq_len, n_features=1, embedding_dim=embedding_dim)\n",
        "    transformer_model = TransformerEncoder(seq_len=seq_len, n_features=1, d_model=embedding_dim)\n",
        "    \n",
        "    # Load weights\n",
        "    lstm_model.load_state_dict(torch.load(os.path.join(base_path, \"lstm_model.pt\")))\n",
        "    gru_model.load_state_dict(torch.load(os.path.join(base_path, \"gru_model.pt\")))\n",
        "    transformer_model.load_state_dict(torch.load(os.path.join(base_path, \"transformer_model.pt\")))\n",
        "    \n",
        "    # Load ensemble weights\n",
        "    with open(os.path.join(base_path, \"ensemble_weights.json\"), \"r\") as f:\n",
        "        weights_dict = json.load(f)\n",
        "    \n",
        "    weights = [weights_dict[\"lstm\"], weights_dict[\"gru\"], weights_dict[\"transformer\"]]\n",
        "    \n",
        "    # Create ensemble model\n",
        "    ensemble_model = EnsembleModel(\n",
        "        models=[lstm_model, gru_model, transformer_model],\n",
        "        names=[\"LSTM\", \"GRU\", \"Transformer\"],\n",
        "        weights=weights\n",
        "    )\n",
        "    \n",
        "    # Load thresholds\n",
        "    with open(os.path.join(base_path, \"thresholds.json\"), \"r\") as f:\n",
        "        thresholds = json.load(f)\n",
        "    \n",
        "    logger.info(f\"Transfer learning models loaded from {base_path}\")\n",
        "    \n",
        "    return {\n",
        "        \"models\": {\n",
        "            \"lstm\": lstm_model,\n",
        "            \"gru\": gru_model,\n",
        "            \"transformer\": transformer_model,\n",
        "            \"ensemble\": ensemble_model\n",
        "        },\n",
        "        \"thresholds\": thresholds,\n",
        "        \"metadata\": metadata\n",
        "    }"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
