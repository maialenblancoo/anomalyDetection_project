{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc78eb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Integrated Anomaly Detection System**\n",
    "# \n",
    "# This notebook integrates all components for a complete anomaly detection system.\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "# Import custom modules\n",
    "import sys\n",
    "sys.path.append('./')\n",
    "from utils.data_loader import load_data, normalize_data, create_sequences\n",
    "from utils.model_utils import get_reconstruction_errors, EnsembleModel, load_trained_models\n",
    "from utils.evaluation import AnomalyInterpreter\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_context(\"notebook\", font_scale=1.2)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logger.info(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "# Load all models\n",
    "def load_all_models():\n",
    "    \"\"\"Load all trained models from various components.\"\"\"\n",
    "    \n",
    "    # Load autoencoder models\n",
    "    autoencoder_models, autoencoder_metadata = load_trained_models(base_path=\"models/autoencoder/\")\n",
    "    \n",
    "    # Load dynamic threshold function\n",
    "    try:\n",
    "        with open(\"models/dynamic_thresholds/hybrid_threshold_model.pkl\", \"rb\") as f:\n",
    "            threshold_model = pickle.load(f)\n",
    "        \n",
    "        def apply_dynamic_threshold(errors, timestamps):\n",
    "            generator = threshold_model['generator']\n",
    "            weights = threshold_model['best_weights']\n",
    "            val_errors = threshold_model['validation_errors']['ensemble']\n",
    "            factor = threshold_model['factor']\n",
    "            return generator.hybrid_threshold(errors, timestamps, val_errors, weights, factor)\n",
    "        \n",
    "        dynamic_threshold_fn = apply_dynamic_threshold\n",
    "    except:\n",
    "        logger.warning(\"Dynamic threshold model not found. Using static threshold.\")\n",
    "        dynamic_threshold_fn = None\n",
    "    \n",
    "    # Load prediction models\n",
    "    try:\n",
    "        from utils.model_utils import load_prediction_models\n",
    "        prediction_models = load_prediction_models(base_path=\"models/prediction/\")\n",
    "    except:\n",
    "        logger.warning(\"Prediction models not found.\")\n",
    "        prediction_models = None\n",
    "    \n",
    "    # Load transfer learning models\n",
    "    try:\n",
    "        # Custom function to load transfer models\n",
    "        def load_transfer_models(base_path=\"models/transfer/\"):\n",
    "            \"\"\"Load transfer learned models.\"\"\"\n",
    "            # Load metadata\n",
    "            with open(os.path.join(base_path, \"transfer_metadata.json\"), \"r\") as f:\n",
    "                metadata = json.load(f)\n",
    "            \n",
    "            # Load thresholds\n",
    "            with open(os.path.join(base_path, \"thresholds.json\"), \"r\") as f:\n",
    "                thresholds = json.load(f)\n",
    "            \n",
    "            # Load ensemble weights\n",
    "            with open(os.path.join(base_path, \"ensemble_weights.json\"), \"r\") as f:\n",
    "                weights_dict = json.load(f)\n",
    "            \n",
    "            return {\n",
    "                \"metadata\": metadata,\n",
    "                \"thresholds\": thresholds,\n",
    "                \"weights\": weights_dict\n",
    "            }\n",
    "        \n",
    "        transfer_models = load_transfer_models(base_path=\"models/transfer/\")\n",
    "    except:\n",
    "        logger.warning(\"Transfer learning models not found.\")\n",
    "        transfer_models = None\n",
    "    \n",
    "    return {\n",
    "        \"autoencoder\": {\n",
    "            \"models\": autoencoder_models,\n",
    "            \"metadata\": autoencoder_metadata\n",
    "        },\n",
    "        \"dynamic_threshold\": dynamic_threshold_fn,\n",
    "        \"prediction\": prediction_models,\n",
    "        \"transfer\": transfer_models\n",
    "    }\n",
    "\n",
    "# Integrated analysis function\n",
    "def analyze_time_series(new_data, models, scaler=None, device='cpu'):\n",
    "    \"\"\"Analyze a time series using all available models.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Preprocess data\n",
    "    if scaler is None:\n",
    "        data_scaled, scaler = normalize_data(new_data)\n",
    "    else:\n",
    "        data_scaled = scaler.transform(new_data.values.reshape(-1, 1))\n",
    "    \n",
    "    # Get sequence length from metadata\n",
    "    sequence_length = models['autoencoder']['metadata']['sequence_length']\n",
    "    \n",
    "    # Create sequences\n",
    "    X = create_sequences(data_scaled, seq_length=sequence_length, step=1)\n",
    "    \n",
    "    # Convert to tensor and create loader\n",
    "    X_tensor = torch.FloatTensor(X)\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        torch.utils.data.TensorDataset(X_tensor),\n",
    "        batch_size=64,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # 1. Detect anomalies using autoencoders\n",
    "    autoencoder_models = models['autoencoder']['models']\n",
    "    \n",
    "    lstm_errors, lstm_orig, lstm_recon = get_reconstruction_errors(\n",
    "        autoencoder_models['lstm'], data_loader, device\n",
    "    )\n",
    "    \n",
    "    gru_errors, gru_orig, gru_recon = get_reconstruction_errors(\n",
    "        autoencoder_models['gru'], data_loader, device\n",
    "    )\n",
    "    \n",
    "    transformer_errors, transformer_orig, transformer_recon = get_reconstruction_errors(\n",
    "        autoencoder_models['transformer'], data_loader, device\n",
    "    )\n",
    "    \n",
    "    # Create ensemble\n",
    "    ensemble = EnsembleModel(\n",
    "        models=[autoencoder_models['lstm'], autoencoder_models['gru'], autoencoder_models['transformer']],\n",
    "        names=[\"LSTM\", \"GRU\", \"Transformer\"]\n",
    "    )\n",
    "    \n",
    "    ensemble_errors, _ = ensemble.get_weighted_errors(\n",
    "        [data_loader] * 3,\n",
    "        device\n",
    "    )\n",
    "    # 2. Apply dynamic thresholds if available\n",
    "    timestamps = pd.date_range(start='2022-01-01', periods=len(ensemble_errors), freq='H')\n",
    "    \n",
    "    if models['dynamic_threshold'] is not None:\n",
    "        dynamic_threshold_fn = models['dynamic_threshold']\n",
    "        thresholds = dynamic_threshold_fn(ensemble_errors, timestamps)\n",
    "    else:\n",
    "        # Use static threshold\n",
    "        val_errors = np.random.choice(ensemble_errors, size=int(len(ensemble_errors)*0.3))\n",
    "        thresholds = np.mean(val_errors) + 1.5 * np.std(val_errors)\n",
    "    \n",
    "    # Detect anomalies\n",
    "    anomalies = ensemble_errors > thresholds\n",
    "    \n",
    "    # Store anomaly detection results\n",
    "    results[\"anomalies\"] = {\n",
    "        \"errors\": {\n",
    "            \"lstm\": lstm_errors,\n",
    "            \"gru\": gru_errors,\n",
    "            \"transformer\": transformer_errors,\n",
    "            \"ensemble\": ensemble_errors\n",
    "        },\n",
    "        \"reconstructions\": {\n",
    "            \"lstm\": (lstm_orig, lstm_recon),\n",
    "            \"gru\": (gru_orig, gru_recon),\n",
    "            \"transformer\": (transformer_orig, transformer_recon)\n",
    "        },\n",
    "        \"thresholds\": thresholds,\n",
    "        \"anomaly_indices\": np.where(anomalies)[0],\n",
    "        \"anomaly_scores\": ensemble_errors / (thresholds if isinstance(thresholds, np.ndarray) else np.array([thresholds] * len(ensemble_errors)))\n",
    "    }\n",
    "    \n",
    "    # 3. Make predictions if models available\n",
    "    if models['prediction'] is not None:\n",
    "        prediction_models = models['prediction']\n",
    "        \n",
    "        # Use most recent data for prediction\n",
    "        if len(data_scaled) >= sequence_length:\n",
    "            recent_data = data_scaled[-sequence_length:].reshape(1, sequence_length, 1)\n",
    "        else:\n",
    "            # If not enough data, pad with zeros\n",
    "            pad_size = sequence_length - len(data_scaled)\n",
    "            padding = np.zeros((pad_size, 1))\n",
    "            recent_data = np.concatenate([padding, data_scaled]).reshape(1, sequence_length, 1)\n",
    "        \n",
    "        # Make predictions with ensemble model\n",
    "        ensemble_predictor = prediction_models['ensemble']\n",
    "        predictions = ensemble_predictor.predict(recent_data, device)\n",
    "        \n",
    "        # Inverse transform predictions\n",
    "        if scaler is not None:\n",
    "            predictions = scaler.inverse_transform(predictions.squeeze()).reshape(-1, 1)\n",
    "        \n",
    "        results[\"predictions\"] = {\n",
    "            \"values\": predictions,\n",
    "            \"horizon\": prediction_models['config']['prediction_horizon']\n",
    "        }\n",
    "    \n",
    "    # 4. Analyze anomalies\n",
    "    interpreter = AnomalyInterpreter(\n",
    "        model=None,\n",
    "        test_data=lstm_orig,\n",
    "        reconstructed_data=lstm_recon,\n",
    "        errors=ensemble_errors,\n",
    "        thresholds=thresholds,\n",
    "        timestamps=timestamps\n",
    "    )\n",
    "    \n",
    "    top_anomalies = interpreter.analyze_top_anomalies(top_k=5)\n",
    "    results[\"analysis\"] = {\n",
    "        \"top_anomalies\": top_anomalies,\n",
    "        \"interpreter\": interpreter\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Function to visualize integrated results\n",
    "def visualize_integrated_results(new_data, results):\n",
    "    \"\"\"Visualize the integrated analysis results.\"\"\"\n",
    "    \n",
    "    # 1. Anomaly detection visualization\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    errors = results[\"anomalies\"][\"errors\"][\"ensemble\"]\n",
    "    thresholds = results[\"anomalies\"][\"thresholds\"]\n",
    "    anomaly_indices = results[\"anomalies\"][\"anomaly_indices\"]\n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(new_data, label='Original Data')\n",
    "    if len(anomaly_indices) > 0:\n",
    "        plt.scatter(anomaly_indices, new_data.iloc[anomaly_indices], color='red', label='Anomalies')\n",
    "    plt.title('Time Series with Anomalies')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(errors, label='Reconstruction Error')\n",
    "    if isinstance(thresholds, np.ndarray):\n",
    "        plt.plot(thresholds, 'r--', label='Dynamic Threshold')\n",
    "    else:\n",
    "        plt.axhline(y=thresholds, color='r', linestyle='--', label=f'Threshold ({thresholds:.6f})')\n",
    "    \n",
    "    if len(anomaly_indices) > 0:\n",
    "        plt.scatter(anomaly_indices, errors[anomaly_indices], color='red')\n",
    "    \n",
    "    plt.title('Reconstruction Error with Threshold')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Error')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('anomaly_detection_results.png', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Show top anomalies\n",
    "    top_anomalies = results[\"analysis\"][\"top_anomalies\"]\n",
    "    interpreter = results[\"analysis\"][\"interpreter\"]\n",
    "    \n",
    "    print(\"\\n=== Top 5 Detected Anomalies ===\")\n",
    "    for i, anomaly in enumerate(top_anomalies):\n",
    "        print(f\"Anomaly #{i+1}:\")\n",
    "        print(f\"  Index: {anomaly['index']}\")\n",
    "        if 'timestamp' in anomaly:\n",
    "            print(f\"  Timestamp: {anomaly['timestamp']}\")\n",
    "        print(f\"  Error: {anomaly['error']:.6f}\")\n",
    "        print(f\"  Threshold: {anomaly['threshold']:.6f}\")\n",
    "        print(f\"  Error/Threshold Ratio: {anomaly['error_ratio']:.4f}\")\n",
    "        \n",
    "        # Visualize the anomaly\n",
    "        interpreter.plot_reconstruction_comparison(anomaly['index'])\n",
    "        plt.savefig(f'anomaly_{i+1}_reconstruction.png', dpi=300)\n",
    "        plt.show()\n",
    "        \n",
    "        interpreter.visualize_contribution_heatmap(anomaly['index'])\n",
    "        plt.savefig(f'anomaly_{i+1}_heatmap.png', dpi=300)\n",
    "        plt.show()\n",
    "    \n",
    "    # 3. Future predictions visualization if available\n",
    "    if \"predictions\" in results:\n",
    "        predictions = results[\"predictions\"][\"values\"]\n",
    "        horizon = results[\"predictions\"][\"horizon\"]\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Plot original data\n",
    "        plt.plot(range(len(new_data)), new_data, label='Historical Data')\n",
    "        \n",
    "        # Plot predictions\n",
    "        pred_indices = range(len(new_data), len(new_data) + len(predictions))\n",
    "        plt.plot(pred_indices, predictions, 'g--', label='Predictions')\n",
    "        \n",
    "        plt.title('Time Series Predictions')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Value')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.savefig('predictions.png', dpi=300)\n",
    "        plt.show()\n",
    "\n",
    "# Main function to demonstrate the integrated system\n",
    "def main():\n",
    "    # Load all models\n",
    "    models = load_all_models()\n",
    "    \n",
    "    # Load a test dataset\n",
    "    filepath = 'machine_temperature_system_failure.csv'\n",
    "    df = load_data(filepath)\n",
    "    \n",
    "    # Take a portion of the data for demonstration\n",
    "    test_data = df[['value']].iloc[-1000:]\n",
    "    \n",
    "    # Analyze the time series\n",
    "    results = analyze_time_series(test_data, models, device=device)\n",
    "    \n",
    "    # Visualize the results\n",
    "    visualize_integrated_results(test_data, results)\n",
    "    \n",
    "    print(\"\\nIntegrated analysis completed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
