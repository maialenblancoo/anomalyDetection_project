{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 0. Directory setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# Get current notebook directory\n",
        "notebook_dir = os.getcwd()\n",
        "\n",
        "# Navigate to project root if notebook is in 'notebooks' folder\n",
        "project_dir = notebook_dir\n",
        "if os.path.basename(notebook_dir) == 'notebooks':\n",
        "    project_dir = os.path.dirname(notebook_dir)\n",
        "    os.chdir(project_dir)\n",
        "\n",
        "# Add project root to sys.path if not already included\n",
        "if project_dir not in sys.path:\n",
        "    sys.path.insert(0, project_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Imports and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import logging\n",
        "\n",
        "# Import custom modules\n",
        "import sys\n",
        "sys.path.append('./')\n",
        "from utils.data_loader import load_data, load_anomaly_labels, normalize_data, create_sequences\n",
        "from utils.data_loader import create_prediction_sequences\n",
        "from utils.model_utils import LSTMPredictor, train_model\n",
        "\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_context(\"notebook\", font_scale=1.2)\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "logger.info(f\"Usando dispositivo: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Data Loading and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Load data\n",
        "filepath = 'data/machine_temperature_system_failure.csv'\n",
        "df = load_data(filepath)\n",
        "\n",
        "data_scaled, scaler = normalize_data(df[['value']])\n",
        "\n",
        "# Create sequences for prediction\n",
        "sequence_length = 150\n",
        "prediction_horizon = 24  # Predict 24 steps ahead\n",
        "\n",
        "X, y = create_prediction_sequences(data_scaled, seq_length=sequence_length, horizon=prediction_horizon)\n",
        "\n",
        "# Split data\n",
        "train_size = int(len(X) * 0.7)\n",
        "val_size = int(len(X) * 0.15)\n",
        "\n",
        "X_train, y_train = X[:train_size], y[:train_size]\n",
        "X_val, y_val = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\n",
        "X_test, y_test = X[train_size+val_size:], y[train_size+val_size:]\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.FloatTensor(X_train)\n",
        "y_train_tensor = torch.FloatTensor(y_train)\n",
        "X_val_tensor = torch.FloatTensor(X_val)\n",
        "y_val_tensor = torch.FloatTensor(y_val)\n",
        "X_test_tensor = torch.FloatTensor(X_test)\n",
        "y_test_tensor = torch.FloatTensor(y_test)\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 64\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Model Definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Define model classes\n",
        "class LSTMMultiStepPredictor(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2):\n",
        "        super(LSTMMultiStepPredictor, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout\n",
        "        )\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        \n",
        "        # Initialize hidden state and cell state\n",
        "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
        "        \n",
        "        # Forward propagate LSTM\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        \n",
        "        # Take the output of the last time step\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        \n",
        "        # Reshape to [batch_size, prediction_horizon, 1]\n",
        "        return out.view(batch_size, -1, 1)\n",
        "\n",
        "class GRUMultiStepPredictor(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2):\n",
        "        super(GRUMultiStepPredictor, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        self.gru = nn.GRU(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout\n",
        "        )\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        \n",
        "        # Initialize hidden state\n",
        "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
        "        \n",
        "        # Forward propagate GRU\n",
        "        out, _ = self.gru(x, h0)\n",
        "        \n",
        "        # Take the output of the last time step\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        \n",
        "        # Reshape to [batch_size, prediction_horizon, 1]\n",
        "        return out.view(batch_size, -1, 1)\n",
        "\n",
        "class TransformerPredictor(nn.Module):\n",
        "    def __init__(self, input_size, d_model, nhead, dim_feedforward, num_layers, output_size, dropout=0.2):\n",
        "        super(TransformerPredictor, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        \n",
        "        self.input_projection = nn.Linear(input_size, d_model)\n",
        "        \n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        \n",
        "        self.output_projection = nn.Linear(d_model, output_size)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        seq_len = x.size(1)\n",
        "        \n",
        "        # Project input to d_model dimensions\n",
        "        x = self.input_projection(x)\n",
        "        \n",
        "        # Create positional encoding\n",
        "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, self.d_model, 2).float() * (-np.log(10000.0) / self.d_model))\n",
        "        pe = torch.zeros(seq_len, self.d_model)\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).to(x.device)\n",
        "        \n",
        "        # Add positional encoding\n",
        "        x = x + pe\n",
        "        \n",
        "        # Forward pass through transformer\n",
        "        x = self.transformer_encoder(x)\n",
        "        \n",
        "        # Take the output of the last time step\n",
        "        x = self.output_projection(x[:, -1, :])\n",
        "        \n",
        "        # Reshape to [batch_size, prediction_horizon, 1]\n",
        "        return x.view(batch_size, -1, 1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Training Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Define training function for prediction models\n",
        "def train_prediction_model(model, train_loader, val_loader, n_epochs=50, learning_rate=1e-3, device='cpu',\n",
        "                         patience=7, min_delta=0.0001, weight_decay=1e-5):\n",
        "    logger.info(f\"Starting prediction model training {model.__class__.__name__}\")\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "    criterion = nn.MSELoss(reduction='mean')\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=3, verbose=True)\n",
        "\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'val_loss': []\n",
        "    }\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    no_improve_count = 0\n",
        "    best_model_state = None\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "\n",
        "        for batch_x, batch_y in train_loader:\n",
        "            batch_x = batch_x.to(device)\n",
        "            batch_y = batch_y.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            outputs = model(batch_x)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "        model.eval()\n",
        "        val_losses = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_x, batch_y in val_loader:\n",
        "                batch_x = batch_x.to(device)\n",
        "                batch_y = batch_y.to(device)\n",
        "                \n",
        "                outputs = model(batch_x)\n",
        "                loss = criterion(outputs, batch_y)\n",
        "                \n",
        "                val_losses.append(loss.item())\n",
        "\n",
        "        train_loss = np.mean(train_losses)\n",
        "        val_loss = np.mean(val_losses)\n",
        "        \n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['val_loss'].append(val_loss)\n",
        "\n",
        "        logger.info(f'Epoch {epoch+1}/{n_epochs}, Train Loss: {train_loss:.5f}, Val Loss: {val_loss:.5f}')\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        if val_loss < best_val_loss - min_delta:\n",
        "            best_val_loss = val_loss\n",
        "            no_improve_count = 0\n",
        "            best_model_state = model.state_dict().copy()\n",
        "        else:\n",
        "            no_improve_count += 1\n",
        "\n",
        "        if no_improve_count >= patience:\n",
        "            logger.info(f'Early stopping triggered after {epoch+1} epochs')\n",
        "            model.load_state_dict(best_model_state)\n",
        "            break\n",
        "\n",
        "    if epoch == n_epochs - 1 and best_model_state is not None:\n",
        "        model.load_state_dict(best_model_state)\n",
        "        logger.info(f'Training completed. Restoring the best model with validation loss: {best_val_loss:.5f}')\n",
        "\n",
        "    return history, model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Model Creation and Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Create models\n",
        "lstm_predictor = LSTMMultiStepPredictor(\n",
        "    input_size=1,\n",
        "    hidden_size=64,\n",
        "    num_layers=2,\n",
        "    output_size=prediction_horizon,\n",
        "    dropout=0.2\n",
        ")\n",
        "\n",
        "gru_predictor = GRUMultiStepPredictor(\n",
        "    input_size=1,\n",
        "    hidden_size=64,\n",
        "    num_layers=2,\n",
        "    output_size=prediction_horizon,\n",
        "    dropout=0.2\n",
        ")\n",
        "\n",
        "transformer_predictor = TransformerPredictor(\n",
        "    input_size=1,\n",
        "    d_model=64,\n",
        "    nhead=4,\n",
        "    dim_feedforward=128,\n",
        "    num_layers=2,\n",
        "    output_size=prediction_horizon,\n",
        "    dropout=0.2\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Train models\n",
        "lstm_history, lstm_model = train_prediction_model(\n",
        "    lstm_predictor, train_loader, val_loader, n_epochs=50,\n",
        "    learning_rate=1e-3, device=device, patience=7\n",
        ")\n",
        "\n",
        "gru_history, gru_model = train_prediction_model(\n",
        "    gru_predictor, train_loader, val_loader, n_epochs=50,\n",
        "    learning_rate=1e-3, device=device, patience=7\n",
        ")\n",
        "\n",
        "transformer_history, transformer_model = train_prediction_model(\n",
        "    transformer_predictor, train_loader, val_loader, n_epochs=50,\n",
        "    learning_rate=1e-3, device=device, patience=7\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Evaluate models\n",
        "def evaluate_prediction_model(model, data_loader, device):\n",
        "    model.eval()\n",
        "    criterion = nn.MSELoss(reduction='mean')\n",
        "    \n",
        "    total_loss = 0\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y in data_loader:\n",
        "            batch_x = batch_x.to(device)\n",
        "            batch_y = batch_y.to(device)\n",
        "            \n",
        "            outputs = model(batch_x)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            \n",
        "            total_loss += loss.item() * batch_x.size(0)\n",
        "            \n",
        "            all_predictions.append(outputs.cpu().numpy())\n",
        "            all_targets.append(batch_y.cpu().numpy())\n",
        "    \n",
        "    all_predictions = np.vstack(all_predictions)\n",
        "    all_targets = np.vstack(all_targets)\n",
        "    \n",
        "    avg_loss = total_loss / len(data_loader.dataset)\n",
        "    \n",
        "    return avg_loss, all_predictions, all_targets\n",
        "\n",
        "lstm_loss, lstm_preds, lstm_targets = evaluate_prediction_model(lstm_model, test_loader, device)\n",
        "gru_loss, gru_preds, gru_targets = evaluate_prediction_model(gru_model, test_loader, device)\n",
        "transformer_loss, transformer_preds, transformer_targets = evaluate_prediction_model(transformer_model, test_loader, device)\n",
        "\n",
        "print(\"\\n=== Prediction Model Evaluation ===\")\n",
        "print(f\"LSTM Test MSE: {lstm_loss:.6f}\")\n",
        "print(f\"GRU Test MSE: {gru_loss:.6f}\")\n",
        "print(f\"Transformer Test MSE: {transformer_loss:.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7. Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Visualize predictions for a sample\n",
        "def plot_prediction_sample(predictions, targets, model_name, sample_idx=0):\n",
        "    pred = predictions[sample_idx].squeeze()\n",
        "    targ = targets[sample_idx].squeeze()\n",
        "    \n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(targ, label='Actual', marker='o')\n",
        "    plt.plot(pred, label='Predicted', marker='x')\n",
        "    plt.title(f'{model_name} - Prediction Sample')\n",
        "    plt.xlabel('Time Step')\n",
        "    plt.ylabel('Normalized Value')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    \n",
        "    return plt.gcf()\n",
        "\n",
        "sample_idx = 10  # Choose a sample to visualize\n",
        "plot_prediction_sample(lstm_preds, lstm_targets, 'LSTM', sample_idx)\n",
        "plt.savefig('lstm_prediction_sample.png', dpi=300)\n",
        "plt.show()\n",
        "\n",
        "plot_prediction_sample(gru_preds, gru_targets, 'GRU', sample_idx)\n",
        "plt.savefig('gru_prediction_sample.png', dpi=300)\n",
        "plt.show()\n",
        "\n",
        "plot_prediction_sample(transformer_preds, transformer_targets, 'Transformer', sample_idx)\n",
        "plt.savefig('transformer_prediction_sample.png', dpi=300)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8. Ensemble Model and Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Create ensemble prediction model\n",
        "class EnsemblePredictionModel:\n",
        "    def __init__(self, models, weights=None):\n",
        "        self.models = models\n",
        "        \n",
        "        if weights is None:\n",
        "            self.weights = np.ones(len(models)) / len(models)\n",
        "        else:\n",
        "            self.weights = np.array(weights) / np.sum(weights)\n",
        "    \n",
        "    def predict(self, x, device='cpu'):\n",
        "        x_tensor = torch.FloatTensor(x).to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            predictions = []\n",
        "            \n",
        "            for i, model in enumerate(self.models):\n",
        "                model.eval()\n",
        "                pred = model(x_tensor).cpu().numpy()\n",
        "                predictions.append(pred)\n",
        "            \n",
        "            # Weighted average of predictions\n",
        "            ensemble_pred = np.zeros_like(predictions[0])\n",
        "            \n",
        "            for i, pred in enumerate(predictions):\n",
        "                ensemble_pred += self.weights[i] * pred\n",
        "            \n",
        "            return ensemble_pred\n",
        "\n",
        "# Optimize ensemble weights\n",
        "def optimize_ensemble_weights(models, val_loader, device):\n",
        "    # Get individual model predictions\n",
        "    all_val_preds = []\n",
        "    \n",
        "    for model in models:\n",
        "        model.eval()\n",
        "        val_preds = []\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for batch_x, _ in val_loader:\n",
        "                batch_x = batch_x.to(device)\n",
        "                outputs = model(batch_x).cpu().numpy()\n",
        "                val_preds.append(outputs)\n",
        "        \n",
        "        val_preds = np.vstack(val_preds)\n",
        "        all_val_preds.append(val_preds)\n",
        "    \n",
        "    # Get validation targets\n",
        "    val_targets = []\n",
        "    for _, batch_y in val_loader:\n",
        "        val_targets.append(batch_y.numpy())\n",
        "    val_targets = np.vstack(val_targets)\n",
        "    \n",
        "    # Grid search for optimal weights\n",
        "    best_mse = float('inf')\n",
        "    best_weights = np.ones(len(models)) / len(models)\n",
        "    \n",
        "    # Generate weight combinations\n",
        "    weight_options = np.linspace(0, 1, 6)  # [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
        "    \n",
        "    if len(models) == 2:\n",
        "        for w1 in weight_options:\n",
        "            w2 = 1.0 - w1\n",
        "            weights = np.array([w1, w2])\n",
        "            \n",
        "            # Calculate weighted prediction\n",
        "            weighted_pred = np.zeros_like(all_val_preds[0])\n",
        "            for i, pred in enumerate(all_val_preds):\n",
        "                weighted_pred += weights[i] * pred\n",
        "            \n",
        "            # Calculate MSE\n",
        "            mse = np.mean((weighted_pred - val_targets) ** 2)\n",
        "            \n",
        "            if mse < best_mse:\n",
        "                best_mse = mse\n",
        "                best_weights = weights\n",
        "    \n",
        "    elif len(models) == 3:\n",
        "        from itertools import product\n",
        "        \n",
        "        for w1, w2 in product(weight_options, weight_options):\n",
        "            w3 = 1.0 - w1 - w2\n",
        "            if 0 <= w3 <= 1:\n",
        "                weights = np.array([w1, w2, w3])\n",
        "                \n",
        "                # Calculate weighted prediction\n",
        "                weighted_pred = np.zeros_like(all_val_preds[0])\n",
        "                for i, pred in enumerate(all_val_preds):\n",
        "                    weighted_pred += weights[i] * pred\n",
        "                \n",
        "                # Calculate MSE\n",
        "                mse = np.mean((weighted_pred - val_targets) ** 2)\n",
        "                \n",
        "                if mse < best_mse:\n",
        "                    best_mse = mse\n",
        "                    best_weights = weights\n",
        "    \n",
        "    print(f\"Best weights found: {best_weights} with MSE: {best_mse:.6f}\")\n",
        "    return best_weights\n",
        "\n",
        "# Get optimal weights\n",
        "best_weights = optimize_ensemble_weights([lstm_model, gru_model, transformer_model], val_loader, device)\n",
        "\n",
        "# Create ensemble model\n",
        "ensemble_prediction_model = EnsemblePredictionModel(\n",
        "    models=[lstm_model, gru_model, transformer_model],\n",
        "    weights=best_weights\n",
        ")\n",
        "\n",
        "# Evaluate ensemble model\n",
        "def evaluate_ensemble_model(ensemble_model, data_loader, device):\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "    \n",
        "    for batch_x, batch_y in data_loader:\n",
        "        batch_x_np = batch_x.numpy()\n",
        "        batch_y_np = batch_y.numpy()\n",
        "        \n",
        "        outputs = ensemble_model.predict(batch_x_np, device)\n",
        "        \n",
        "        all_predictions.append(outputs)\n",
        "        all_targets.append(batch_y_np)\n",
        "    \n",
        "    all_predictions = np.vstack(all_predictions)\n",
        "    all_targets = np.vstack(all_targets)\n",
        "    \n",
        "    mse = np.mean((all_predictions - all_targets) ** 2)\n",
        "    \n",
        "    return mse, all_predictions, all_targets\n",
        "\n",
        "# Evaluate ensemble model\n",
        "ensemble_loss, ensemble_preds, ensemble_targets = evaluate_ensemble_model(\n",
        "    ensemble_prediction_model, test_loader, device\n",
        ")\n",
        "\n",
        "print(f\"\\nEnsemble Test MSE: {ensemble_loss:.6f}\")\n",
        "\n",
        "# Visualize ensemble prediction\n",
        "plot_prediction_sample(ensemble_preds, ensemble_targets, 'Ensemble', sample_idx)\n",
        "plt.savefig('ensemble_prediction_sample.png', dpi=300)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9. Model Comparison and Error Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Compare all models\n",
        "print(\"\\n=== Model Comparison ===\")\n",
        "print(f\"{'Model':<12} {'Test MSE':<12}\")\n",
        "print(\"-\" * 25)\n",
        "print(f\"{'LSTM':<12} {lstm_loss:<12.6f}\")\n",
        "print(f\"{'GRU':<12} {gru_loss:<12.6f}\")\n",
        "print(f\"{'Transformer':<12} {transformer_loss:<12.6f}\")\n",
        "print(f\"{'Ensemble':<12} {ensemble_loss:<12.6f}\")\n",
        "\n",
        "# Plot comparison of all models for a sample\n",
        "def plot_all_models_comparison(predictions_dict, targets, sample_idx=0):\n",
        "    plt.figure(figsize=(14, 7))\n",
        "    \n",
        "    target = targets[sample_idx].squeeze()\n",
        "    plt.plot(target, 'k-', lw=2, label='Actual')\n",
        "    \n",
        "    for model_name, preds in predictions_dict.items():\n",
        "        pred = preds[sample_idx].squeeze()\n",
        "        plt.plot(pred, '--', label=f'{model_name}')\n",
        "    \n",
        "    plt.title('Model Comparison - Prediction Sample')\n",
        "    plt.xlabel('Time Step')\n",
        "    plt.ylabel('Normalized Value')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    \n",
        "    return plt.gcf()\n",
        "\n",
        "# Compare all models on a sample\n",
        "predictions_dict = {\n",
        "    'LSTM': lstm_preds,\n",
        "    'GRU': gru_preds,\n",
        "    'Transformer': transformer_preds,\n",
        "    'Ensemble': ensemble_preds\n",
        "}\n",
        "\n",
        "plot_all_models_comparison(predictions_dict, ensemble_targets, sample_idx)\n",
        "plt.savefig('model_comparison.png', dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# Calculate prediction errors\n",
        "def calculate_prediction_errors(predictions, targets):\n",
        "    return np.mean(np.abs(predictions - targets), axis=2).squeeze()\n",
        "\n",
        "lstm_errors = calculate_prediction_errors(lstm_preds, lstm_targets)\n",
        "gru_errors = calculate_prediction_errors(gru_preds, gru_targets)\n",
        "transformer_errors = calculate_prediction_errors(transformer_preds, transformer_targets)\n",
        "ensemble_errors = calculate_prediction_errors(ensemble_preds, ensemble_targets)\n",
        "\n",
        "# Plot error distribution\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.hist(lstm_errors, bins=50, alpha=0.5, label='LSTM')\n",
        "plt.hist(gru_errors, bins=50, alpha=0.5, label='GRU')\n",
        "plt.hist(transformer_errors, bins=50, alpha=0.5, label='Transformer')\n",
        "plt.hist(ensemble_errors, bins=50, alpha=0.5, label='Ensemble')\n",
        "\n",
        "plt.title('Error Distribution by Model')\n",
        "plt.xlabel('Mean Absolute Error')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.savefig('error_distribution.png', dpi=300)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 10. Model Saving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Save models\n",
        "def save_prediction_models(models_dict, weights=None, base_path=\"models/prediction/\"):\n",
        "    import os\n",
        "    import json\n",
        "    \n",
        "    os.makedirs(base_path, exist_ok=True)\n",
        "    \n",
        "    for name, model in models_dict.items():\n",
        "        model_path = os.path.join(base_path, f\"{name}_model.pt\")\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "        logger.info(f\"Modelo {name} guardado en {model_path}\")\n",
        "    \n",
        "    if weights is not None:\n",
        "        # Save ensemble weights\n",
        "        weights_dict = {\n",
        "            \"lstm\": float(weights[0]),\n",
        "            \"gru\": float(weights[1]),\n",
        "            \"transformer\": float(weights[2])\n",
        "        }\n",
        "        \n",
        "        with open(os.path.join(base_path, \"ensemble_weights.json\"), \"w\") as f:\n",
        "            json.dump(weights_dict, f)\n",
        "        \n",
        "        logger.info(f\"Ensemble weights saved to {os.path.join(base_path, 'ensemble_weights.json')}\")\n",
        "    \n",
        "    # Save configuration\n",
        "    config = {\n",
        "        \"sequence_length\": sequence_length,\n",
        "        \"prediction_horizon\": prediction_horizon,\n",
        "        \"embedding_dim\": 64,\n",
        "        \"input_size\": 1\n",
        "    }\n",
        "    \n",
        "    with open(os.path.join(base_path, \"model_config.json\"), \"w\") as f:\n",
        "        json.dump(config, f)\n",
        "    \n",
        "    logger.info(f\"Configuration saved to {os.path.join(base_path, 'model_config.json')}\")\n",
        "\n",
        "# Save prediction models\n",
        "save_prediction_models(\n",
        "    models_dict={\n",
        "        \"lstm\": lstm_model,\n",
        "        \"gru\": gru_model,\n",
        "        \"transformer\": transformer_model\n",
        "    },\n",
        "    weights=best_weights,\n",
        "    base_path=\"models/prediction/\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 11. Model Loading and Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def load_prediction_models(base_path=\"models/prediction/\"):\n",
        "    import os\n",
        "    import json\n",
        "    \n",
        "    # Load configuration\n",
        "    with open(os.path.join(base_path, \"model_config.json\"), \"r\") as f:\n",
        "        config = json.load(f)\n",
        "    \n",
        "    sequence_length = config[\"sequence_length\"]\n",
        "    prediction_horizon = config[\"prediction_horizon\"]\n",
        "    embedding_dim = config[\"embedding_dim\"]\n",
        "    input_size = config[\"input_size\"]\n",
        "    \n",
        "    # Initialize models\n",
        "    lstm_model = LSTMMultiStepPredictor(\n",
        "        input_size=input_size,\n",
        "        hidden_size=embedding_dim,\n",
        "        num_layers=2,\n",
        "        output_size=prediction_horizon,\n",
        "        dropout=0.2\n",
        "    )\n",
        "    \n",
        "    gru_model = GRUMultiStepPredictor(\n",
        "        input_size=input_size,\n",
        "        hidden_size=embedding_dim,\n",
        "        num_layers=2,\n",
        "        output_size=prediction_horizon,\n",
        "        dropout=0.2\n",
        "    )\n",
        "    \n",
        "    transformer_model = TransformerPredictor(\n",
        "        input_size=input_size,\n",
        "        d_model=embedding_dim,\n",
        "        nhead=4,\n",
        "        dim_feedforward=128,\n",
        "        num_layers=2,\n",
        "        output_size=prediction_horizon,\n",
        "        dropout=0.2\n",
        "    )\n",
        "    \n",
        "    # Load model weights\n",
        "    lstm_model.load_state_dict(torch.load(os.path.join(base_path, \"lstm_model.pt\")))\n",
        "    gru_model.load_state_dict(torch.load(os.path.join(base_path, \"gru_model.pt\")))\n",
        "    transformer_model.load_state_dict(torch.load(os.path.join(base_path, \"transformer_model.pt\")))\n",
        "    \n",
        "    # Load ensemble weights\n",
        "    with open(os.path.join(base_path, \"ensemble_weights.json\"), \"r\") as f:\n",
        "        weights_dict = json.load(f)\n",
        "    \n",
        "    weights = [weights_dict[\"lstm\"], weights_dict[\"gru\"], weights_dict[\"transformer\"]]\n",
        "    \n",
        "    # Create ensemble model\n",
        "    ensemble_model = EnsemblePredictionModel(\n",
        "        models=[lstm_model, gru_model, transformer_model],\n",
        "        weights=weights\n",
        "    )\n",
        "    \n",
        "    logger.info(f\"Prediction models loaded from {base_path}\")\n",
        "    \n",
        "    return {\n",
        "        \"lstm\": lstm_model,\n",
        "        \"gru\": gru_model,\n",
        "        \"transformer\": transformer_model,\n",
        "        \"ensemble\": ensemble_model,\n",
        "        \"config\": config\n",
        "    }\n",
        "\n",
        "# Function to make predictions with the saved models\n",
        "def make_predictions(new_data, models, scaler=None, device='cpu'):\n",
        "    # Preprocess data\n",
        "    if scaler is not None and not isinstance(new_data, np.ndarray):\n",
        "        new_data = scaler.transform(new_data.values.reshape(-1, 1))\n",
        "    \n",
        "    # Create sequences\n",
        "    config = models[\"config\"]\n",
        "    sequence_length = config[\"sequence_length\"]\n",
        "    \n",
        "    # Create sequence from most recent data\n",
        "    if len(new_data) >= sequence_length:\n",
        "        recent_data = new_data[-sequence_length:].reshape(1, sequence_length, 1)\n",
        "    else:\n",
        "        # If not enough data, pad with zeros\n",
        "        pad_size = sequence_length - len(new_data)\n",
        "        padding = np.zeros((pad_size, 1))\n",
        "        recent_data = np.concatenate([padding, new_data]).reshape(1, sequence_length, 1)\n",
        "    \n",
        "    # Make predictions\n",
        "    ensemble_model = models[\"ensemble\"]\n",
        "    predictions = ensemble_model.predict(recent_data, device)\n",
        "    \n",
        "    # Inverse transform if scaler provided\n",
        "    if scaler is not None:\n",
        "        predictions = scaler.inverse_transform(predictions.squeeze()).reshape(-1, 1)\n",
        "    \n",
        "    return predictions"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
