{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Directorio actual: c:\\Users\\aleja\\Desktop\\deepProject\\anomalyDetection_project\\notebooks\n",
            "Cambiado a directorio: c:\\Users\\aleja\\Desktop\\deepProject\\anomalyDetection_project\n",
            "Añadido al path: c:\\Users\\aleja\\Desktop\\deepProject\\anomalyDetection_project\n",
            "Contenido del directorio raíz: ['.git', 'data', 'hybrid_threshold.png', 'models', 'notebooks', 'results', 'threshold_comparison.png', 'utils']\n",
            "Contenido de utils: ['data_loader.py', 'evaluation.py', 'model_utils.py', '__pycache__']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# Obtener el directorio actual del notebook\n",
        "notebook_dir = os.getcwd()\n",
        "print(\"Directorio actual:\", notebook_dir)\n",
        "\n",
        "# Ir al directorio raíz del proyecto (si el notebook está en la carpeta notebooks)\n",
        "project_dir = notebook_dir\n",
        "if os.path.basename(notebook_dir) == 'notebooks':\n",
        "    project_dir = os.path.dirname(notebook_dir)\n",
        "    os.chdir(project_dir)\n",
        "    print(\"Cambiado a directorio:\", os.getcwd())\n",
        "else:\n",
        "    print(\"Ya parece estar en el directorio raíz\")\n",
        "\n",
        "# Añadir el directorio raíz al path\n",
        "if project_dir not in sys.path:\n",
        "    sys.path.insert(0, project_dir)\n",
        "    print(\"Añadido al path:\", project_dir)\n",
        "else:\n",
        "    print(\"El directorio ya está en el path\")\n",
        "\n",
        "# Verificar que utils esté accesible\n",
        "print(\"Contenido del directorio raíz:\", os.listdir())\n",
        "print(\"Contenido de utils:\", os.listdir('utils') if os.path.exists('utils') else \"La carpeta utils no existe\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jZUT3EtJX8SF"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-14 18:52:11,488 - INFO - Usando dispositivo: cpu\n",
            "2025-05-14 18:52:11,489 - INFO - Charging data from data/machine_temperature_system_failure.csv\n",
            "2025-05-14 18:52:11,509 - INFO - Charged data: 22695 registers\n",
            "2025-05-14 18:52:11,544 - INFO - Starting prediction model training LSTMMultiStepPredictor\n",
            "C:\\Users\\aleja\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "2025-05-14 18:52:35,737 - INFO - Epoch 1/50, Train Loss: 0.04708, Val Loss: 0.01217\n",
            "2025-05-14 18:52:59,212 - INFO - Epoch 2/50, Train Loss: 0.00306, Val Loss: 0.00536\n",
            "2025-05-14 18:53:23,284 - INFO - Epoch 3/50, Train Loss: 0.00233, Val Loss: 0.00313\n",
            "2025-05-14 18:53:48,141 - INFO - Epoch 4/50, Train Loss: 0.00217, Val Loss: 0.00320\n",
            "2025-05-14 18:54:13,617 - INFO - Epoch 5/50, Train Loss: 0.00206, Val Loss: 0.00315\n",
            "2025-05-14 18:54:38,982 - INFO - Epoch 6/50, Train Loss: 0.00200, Val Loss: 0.00311\n",
            "2025-05-14 18:55:04,478 - INFO - Epoch 7/50, Train Loss: 0.00192, Val Loss: 0.00269\n",
            "2025-05-14 18:55:30,367 - INFO - Epoch 8/50, Train Loss: 0.00181, Val Loss: 0.00258\n",
            "2025-05-14 18:55:56,536 - INFO - Epoch 9/50, Train Loss: 0.00179, Val Loss: 0.00321\n",
            "2025-05-14 18:56:22,464 - INFO - Epoch 10/50, Train Loss: 0.00175, Val Loss: 0.00295\n",
            "2025-05-14 18:56:48,344 - INFO - Epoch 11/50, Train Loss: 0.00173, Val Loss: 0.00288\n",
            "2025-05-14 18:57:14,238 - INFO - Epoch 12/50, Train Loss: 0.00173, Val Loss: 0.00268\n",
            "2025-05-14 18:57:40,782 - INFO - Epoch 13/50, Train Loss: 0.00164, Val Loss: 0.00237\n",
            "2025-05-14 18:58:07,302 - INFO - Epoch 14/50, Train Loss: 0.00161, Val Loss: 0.00240\n",
            "2025-05-14 18:58:33,704 - INFO - Epoch 15/50, Train Loss: 0.00164, Val Loss: 0.00230\n",
            "2025-05-14 18:59:00,415 - INFO - Epoch 16/50, Train Loss: 0.00161, Val Loss: 0.00233\n",
            "2025-05-14 18:59:27,407 - INFO - Epoch 17/50, Train Loss: 0.00161, Val Loss: 0.00244\n",
            "2025-05-14 18:59:54,249 - INFO - Epoch 18/50, Train Loss: 0.00158, Val Loss: 0.00242\n",
            "2025-05-14 19:00:20,583 - INFO - Epoch 19/50, Train Loss: 0.00160, Val Loss: 0.00246\n",
            "2025-05-14 19:00:47,375 - INFO - Epoch 20/50, Train Loss: 0.00154, Val Loss: 0.00253\n",
            "2025-05-14 19:00:47,377 - INFO - Early stopping triggered after 20 epochs\n",
            "2025-05-14 19:00:47,378 - INFO - Starting prediction model training GRUMultiStepPredictor\n",
            "2025-05-14 19:01:30,190 - INFO - Epoch 1/50, Train Loss: 0.03444, Val Loss: 0.00353\n",
            "2025-05-14 19:02:13,313 - INFO - Epoch 2/50, Train Loss: 0.00224, Val Loss: 0.00325\n",
            "2025-05-14 19:02:55,834 - INFO - Epoch 3/50, Train Loss: 0.00212, Val Loss: 0.00286\n",
            "2025-05-14 19:03:41,427 - INFO - Epoch 4/50, Train Loss: 0.00197, Val Loss: 0.00294\n",
            "2025-05-14 19:04:23,555 - INFO - Epoch 5/50, Train Loss: 0.00191, Val Loss: 0.00247\n",
            "2025-05-14 19:05:08,803 - INFO - Epoch 6/50, Train Loss: 0.00175, Val Loss: 0.00263\n",
            "2025-05-14 19:05:55,624 - INFO - Epoch 7/50, Train Loss: 0.00173, Val Loss: 0.00265\n",
            "2025-05-14 19:06:48,024 - INFO - Epoch 8/50, Train Loss: 0.00162, Val Loss: 0.00311\n",
            "2025-05-14 19:07:36,194 - INFO - Epoch 9/50, Train Loss: 0.00162, Val Loss: 0.00300\n",
            "2025-05-14 19:08:23,907 - INFO - Epoch 10/50, Train Loss: 0.00156, Val Loss: 0.00230\n",
            "2025-05-14 19:09:22,961 - INFO - Epoch 11/50, Train Loss: 0.00154, Val Loss: 0.00224\n",
            "2025-05-14 19:10:27,025 - INFO - Epoch 12/50, Train Loss: 0.00150, Val Loss: 0.00260\n",
            "2025-05-14 19:11:24,374 - INFO - Epoch 13/50, Train Loss: 0.00150, Val Loss: 0.00236\n",
            "2025-05-14 19:12:22,326 - INFO - Epoch 14/50, Train Loss: 0.00150, Val Loss: 0.00244\n",
            "2025-05-14 19:13:21,661 - INFO - Epoch 15/50, Train Loss: 0.00150, Val Loss: 0.00222\n",
            "2025-05-14 19:14:20,048 - INFO - Epoch 16/50, Train Loss: 0.00149, Val Loss: 0.00227\n",
            "2025-05-14 19:15:31,445 - INFO - Epoch 17/50, Train Loss: 0.00147, Val Loss: 0.00210\n",
            "2025-05-14 19:17:00,063 - INFO - Epoch 18/50, Train Loss: 0.00145, Val Loss: 0.00231\n",
            "2025-05-14 19:18:18,416 - INFO - Epoch 19/50, Train Loss: 0.00146, Val Loss: 0.00250\n",
            "2025-05-14 19:20:56,547 - INFO - Epoch 20/50, Train Loss: 0.00145, Val Loss: 0.00206\n",
            "2025-05-14 19:23:22,738 - INFO - Epoch 21/50, Train Loss: 0.00144, Val Loss: 0.00218\n",
            "2025-05-14 19:24:40,929 - INFO - Epoch 22/50, Train Loss: 0.00142, Val Loss: 0.00224\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 291\u001b[39m\n\u001b[32m    285\u001b[39m \u001b[38;5;66;03m# Train models\u001b[39;00m\n\u001b[32m    286\u001b[39m lstm_history, lstm_model = train_prediction_model(\n\u001b[32m    287\u001b[39m     lstm_predictor, train_loader, val_loader, n_epochs=\u001b[32m50\u001b[39m,\n\u001b[32m    288\u001b[39m     learning_rate=\u001b[32m1e-3\u001b[39m, device=device, patience=\u001b[32m7\u001b[39m\n\u001b[32m    289\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m gru_history, gru_model = \u001b[43mtrain_prediction_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgru_predictor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m7\u001b[39;49m\n\u001b[32m    294\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    296\u001b[39m transformer_history, transformer_model = train_prediction_model(\n\u001b[32m    297\u001b[39m     transformer_predictor, train_loader, val_loader, n_epochs=\u001b[32m50\u001b[39m,\n\u001b[32m    298\u001b[39m     learning_rate=\u001b[32m1e-3\u001b[39m, device=device, patience=\u001b[32m7\u001b[39m\n\u001b[32m    299\u001b[39m )\n\u001b[32m    301\u001b[39m \u001b[38;5;66;03m# Evaluate models\u001b[39;00m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 212\u001b[39m, in \u001b[36mtrain_prediction_model\u001b[39m\u001b[34m(model, train_loader, val_loader, n_epochs, learning_rate, device, patience, min_delta, weight_decay)\u001b[39m\n\u001b[32m    209\u001b[39m outputs = model(batch_x)\n\u001b[32m    210\u001b[39m loss = criterion(outputs, batch_y)\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    213\u001b[39m optimizer.step()\n\u001b[32m    215\u001b[39m train_losses.append(loss.item())\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\autograd\\__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\autograd\\graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# **Prediction Models for Time Series Forecasting**\n",
        "# \n",
        "# This notebook implements predictive models for time series forecasting.\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import logging\n",
        "\n",
        "# Import custom modules\n",
        "import sys\n",
        "sys.path.append('./')\n",
        "from utils.data_loader import load_data, load_anomaly_labels, normalize_data, create_sequences\n",
        "from utils.data_loader import create_prediction_sequences\n",
        "from utils.model_utils import LSTMPredictor, train_model\n",
        "\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_context(\"notebook\", font_scale=1.2)\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "logger.info(f\"Usando dispositivo: {device}\")\n",
        "\n",
        "# Load data\n",
        "filepath = 'data/machine_temperature_system_failure.csv'\n",
        "df = load_data(filepath)\n",
        "\n",
        "data_scaled, scaler = normalize_data(df[['value']])\n",
        "\n",
        "# Create sequences for prediction\n",
        "sequence_length = 150\n",
        "prediction_horizon = 24  # Predict 24 steps ahead\n",
        "\n",
        "X, y = create_prediction_sequences(data_scaled, seq_length=sequence_length, horizon=prediction_horizon)\n",
        "\n",
        "# Split data\n",
        "train_size = int(len(X) * 0.7)\n",
        "val_size = int(len(X) * 0.15)\n",
        "\n",
        "X_train, y_train = X[:train_size], y[:train_size]\n",
        "X_val, y_val = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\n",
        "X_test, y_test = X[train_size+val_size:], y[train_size+val_size:]\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.FloatTensor(X_train)\n",
        "y_train_tensor = torch.FloatTensor(y_train)\n",
        "X_val_tensor = torch.FloatTensor(X_val)\n",
        "y_val_tensor = torch.FloatTensor(y_val)\n",
        "X_test_tensor = torch.FloatTensor(X_test)\n",
        "y_test_tensor = torch.FloatTensor(y_test)\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 64\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Define model classes\n",
        "class LSTMMultiStepPredictor(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2):\n",
        "        super(LSTMMultiStepPredictor, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout\n",
        "        )\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        \n",
        "        # Initialize hidden state and cell state\n",
        "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
        "        \n",
        "        # Forward propagate LSTM\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        \n",
        "        # Take the output of the last time step\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        \n",
        "        # Reshape to [batch_size, prediction_horizon, 1]\n",
        "        return out.view(batch_size, -1, 1)\n",
        "\n",
        "class GRUMultiStepPredictor(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2):\n",
        "        super(GRUMultiStepPredictor, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        self.gru = nn.GRU(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout\n",
        "        )\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        \n",
        "        # Initialize hidden state\n",
        "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
        "        \n",
        "        # Forward propagate GRU\n",
        "        out, _ = self.gru(x, h0)\n",
        "        \n",
        "        # Take the output of the last time step\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        \n",
        "        # Reshape to [batch_size, prediction_horizon, 1]\n",
        "        return out.view(batch_size, -1, 1)\n",
        "\n",
        "class TransformerPredictor(nn.Module):\n",
        "    def __init__(self, input_size, d_model, nhead, dim_feedforward, num_layers, output_size, dropout=0.2):\n",
        "        super(TransformerPredictor, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        \n",
        "        self.input_projection = nn.Linear(input_size, d_model)\n",
        "        \n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        \n",
        "        self.output_projection = nn.Linear(d_model, output_size)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        seq_len = x.size(1)\n",
        "        \n",
        "        # Project input to d_model dimensions\n",
        "        x = self.input_projection(x)\n",
        "        \n",
        "        # Create positional encoding\n",
        "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, self.d_model, 2).float() * (-np.log(10000.0) / self.d_model))\n",
        "        pe = torch.zeros(seq_len, self.d_model)\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).to(x.device)\n",
        "        \n",
        "        # Add positional encoding\n",
        "        x = x + pe\n",
        "        \n",
        "        # Forward pass through transformer\n",
        "        x = self.transformer_encoder(x)\n",
        "        \n",
        "        # Take the output of the last time step\n",
        "        x = self.output_projection(x[:, -1, :])\n",
        "        \n",
        "        # Reshape to [batch_size, prediction_horizon, 1]\n",
        "        return x.view(batch_size, -1, 1)\n",
        "\n",
        "# Define training function for prediction models\n",
        "def train_prediction_model(model, train_loader, val_loader, n_epochs=50, learning_rate=1e-3, device='cpu',\n",
        "                         patience=7, min_delta=0.0001, weight_decay=1e-5):\n",
        "    logger.info(f\"Starting prediction model training {model.__class__.__name__}\")\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "    criterion = nn.MSELoss(reduction='mean')\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=3, verbose=True)\n",
        "\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'val_loss': []\n",
        "    }\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    no_improve_count = 0\n",
        "    best_model_state = None\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "\n",
        "        for batch_x, batch_y in train_loader:\n",
        "            batch_x = batch_x.to(device)\n",
        "            batch_y = batch_y.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            outputs = model(batch_x)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "        model.eval()\n",
        "        val_losses = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_x, batch_y in val_loader:\n",
        "                batch_x = batch_x.to(device)\n",
        "                batch_y = batch_y.to(device)\n",
        "                \n",
        "                outputs = model(batch_x)\n",
        "                loss = criterion(outputs, batch_y)\n",
        "                \n",
        "                val_losses.append(loss.item())\n",
        "\n",
        "        train_loss = np.mean(train_losses)\n",
        "        val_loss = np.mean(val_losses)\n",
        "        \n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['val_loss'].append(val_loss)\n",
        "\n",
        "        logger.info(f'Epoch {epoch+1}/{n_epochs}, Train Loss: {train_loss:.5f}, Val Loss: {val_loss:.5f}')\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        if val_loss < best_val_loss - min_delta:\n",
        "            best_val_loss = val_loss\n",
        "            no_improve_count = 0\n",
        "            best_model_state = model.state_dict().copy()\n",
        "        else:\n",
        "            no_improve_count += 1\n",
        "\n",
        "        if no_improve_count >= patience:\n",
        "            logger.info(f'Early stopping triggered after {epoch+1} epochs')\n",
        "            model.load_state_dict(best_model_state)\n",
        "            break\n",
        "\n",
        "    if epoch == n_epochs - 1 and best_model_state is not None:\n",
        "        model.load_state_dict(best_model_state)\n",
        "        logger.info(f'Training completed. Restoring the best model with validation loss: {best_val_loss:.5f}')\n",
        "\n",
        "    return history, model\n",
        "\n",
        "# Create models\n",
        "lstm_predictor = LSTMMultiStepPredictor(\n",
        "    input_size=1,\n",
        "    hidden_size=64,\n",
        "    num_layers=2,\n",
        "    output_size=prediction_horizon,\n",
        "    dropout=0.2\n",
        ")\n",
        "\n",
        "gru_predictor = GRUMultiStepPredictor(\n",
        "    input_size=1,\n",
        "    hidden_size=64,\n",
        "    num_layers=2,\n",
        "    output_size=prediction_horizon,\n",
        "    dropout=0.2\n",
        ")\n",
        "\n",
        "transformer_predictor = TransformerPredictor(\n",
        "    input_size=1,\n",
        "    d_model=64,\n",
        "    nhead=4,\n",
        "    dim_feedforward=128,\n",
        "    num_layers=2,\n",
        "    output_size=prediction_horizon,\n",
        "    dropout=0.2\n",
        ")\n",
        "\n",
        "# Train models\n",
        "lstm_history, lstm_model = train_prediction_model(\n",
        "    lstm_predictor, train_loader, val_loader, n_epochs=50,\n",
        "    learning_rate=1e-3, device=device, patience=7\n",
        ")\n",
        "\n",
        "gru_history, gru_model = train_prediction_model(\n",
        "    gru_predictor, train_loader, val_loader, n_epochs=50,\n",
        "    learning_rate=1e-3, device=device, patience=7\n",
        ")\n",
        "\n",
        "transformer_history, transformer_model = train_prediction_model(\n",
        "    transformer_predictor, train_loader, val_loader, n_epochs=50,\n",
        "    learning_rate=1e-3, device=device, patience=7\n",
        ")\n",
        "\n",
        "# Evaluate models\n",
        "def evaluate_prediction_model(model, data_loader, device):\n",
        "    model.eval()\n",
        "    criterion = nn.MSELoss(reduction='mean')\n",
        "    \n",
        "    total_loss = 0\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y in data_loader:\n",
        "            batch_x = batch_x.to(device)\n",
        "            batch_y = batch_y.to(device)\n",
        "            \n",
        "            outputs = model(batch_x)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            \n",
        "            total_loss += loss.item() * batch_x.size(0)\n",
        "            \n",
        "            all_predictions.append(outputs.cpu().numpy())\n",
        "            all_targets.append(batch_y.cpu().numpy())\n",
        "    \n",
        "    all_predictions = np.vstack(all_predictions)\n",
        "    all_targets = np.vstack(all_targets)\n",
        "    \n",
        "    avg_loss = total_loss / len(data_loader.dataset)\n",
        "    \n",
        "    return avg_loss, all_predictions, all_targets\n",
        "\n",
        "lstm_loss, lstm_preds, lstm_targets = evaluate_prediction_model(lstm_model, test_loader, device)\n",
        "gru_loss, gru_preds, gru_targets = evaluate_prediction_model(gru_model, test_loader, device)\n",
        "transformer_loss, transformer_preds, transformer_targets = evaluate_prediction_model(transformer_model, test_loader, device)\n",
        "\n",
        "print(\"\\n=== Prediction Model Evaluation ===\")\n",
        "print(f\"LSTM Test MSE: {lstm_loss:.6f}\")\n",
        "print(f\"GRU Test MSE: {gru_loss:.6f}\")\n",
        "print(f\"Transformer Test MSE: {transformer_loss:.6f}\")\n",
        "\n",
        "# Visualize predictions for a sample\n",
        "def plot_prediction_sample(predictions, targets, model_name, sample_idx=0):\n",
        "    pred = predictions[sample_idx].squeeze()\n",
        "    targ = targets[sample_idx].squeeze()\n",
        "    \n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(targ, label='Actual', marker='o')\n",
        "    plt.plot(pred, label='Predicted', marker='x')\n",
        "    plt.title(f'{model_name} - Prediction Sample')\n",
        "    plt.xlabel('Time Step')\n",
        "    plt.ylabel('Normalized Value')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    \n",
        "    return plt.gcf()\n",
        "\n",
        "sample_idx = 10  # Choose a sample to visualize\n",
        "plot_prediction_sample(lstm_preds, lstm_targets, 'LSTM', sample_idx)\n",
        "plt.savefig('lstm_prediction_sample.png', dpi=300)\n",
        "plt.show()\n",
        "\n",
        "plot_prediction_sample(gru_preds, gru_targets, 'GRU', sample_idx)\n",
        "plt.savefig('gru_prediction_sample.png', dpi=300)\n",
        "plt.show()\n",
        "\n",
        "plot_prediction_sample(transformer_preds, transformer_targets, 'Transformer', sample_idx)\n",
        "plt.savefig('transformer_prediction_sample.png', dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# Create ensemble prediction model\n",
        "class EnsemblePredictionModel:\n",
        "    def __init__(self, models, weights=None):\n",
        "        self.models = models\n",
        "        \n",
        "        if weights is None:\n",
        "            self.weights = np.ones(len(models)) / len(models)\n",
        "        else:\n",
        "            self.weights = np.array(weights) / np.sum(weights)\n",
        "    \n",
        "    def predict(self, x, device='cpu'):\n",
        "        x_tensor = torch.FloatTensor(x).to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            predictions = []\n",
        "            \n",
        "            for i, model in enumerate(self.models):\n",
        "                model.eval()\n",
        "                pred = model(x_tensor).cpu().numpy()\n",
        "                predictions.append(pred)\n",
        "            \n",
        "            # Weighted average of predictions\n",
        "            ensemble_pred = np.zeros_like(predictions[0])\n",
        "            \n",
        "            for i, pred in enumerate(predictions):\n",
        "                ensemble_pred += self.weights[i] * pred\n",
        "            \n",
        "            return ensemble_pred\n",
        "\n",
        "# Optimize ensemble weights\n",
        "def optimize_ensemble_weights(models, val_loader, device):\n",
        "    # Get individual model predictions\n",
        "    all_val_preds = []\n",
        "    \n",
        "    for model in models:\n",
        "        model.eval()\n",
        "        val_preds = []\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for batch_x, _ in val_loader:\n",
        "                batch_x = batch_x.to(device)\n",
        "                outputs = model(batch_x).cpu().numpy()\n",
        "                val_preds.append(outputs)\n",
        "        \n",
        "        val_preds = np.vstack(val_preds)\n",
        "        all_val_preds.append(val_preds)\n",
        "    \n",
        "    # Get validation targets\n",
        "    val_targets = []\n",
        "    for _, batch_y in val_loader:\n",
        "        val_targets.append(batch_y.numpy())\n",
        "    val_targets = np.vstack(val_targets)\n",
        "    \n",
        "    # Grid search for optimal weights\n",
        "    best_mse = float('inf')\n",
        "    best_weights = np.ones(len(models)) / len(models)\n",
        "    \n",
        "    # Generate weight combinations\n",
        "    weight_options = np.linspace(0, 1, 6)  # [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
        "    \n",
        "    if len(models) == 2:\n",
        "        for w1 in weight_options:\n",
        "            w2 = 1.0 - w1\n",
        "            weights = np.array([w1, w2])\n",
        "            \n",
        "            # Calculate weighted prediction\n",
        "            weighted_pred = np.zeros_like(all_val_preds[0])\n",
        "            for i, pred in enumerate(all_val_preds):\n",
        "                weighted_pred += weights[i] * pred\n",
        "            \n",
        "            # Calculate MSE\n",
        "            mse = np.mean((weighted_pred - val_targets) ** 2)\n",
        "            \n",
        "            if mse < best_mse:\n",
        "                best_mse = mse\n",
        "                best_weights = weights\n",
        "    \n",
        "    elif len(models) == 3:\n",
        "        from itertools import product\n",
        "        \n",
        "        for w1, w2 in product(weight_options, weight_options):\n",
        "            w3 = 1.0 - w1 - w2\n",
        "            if 0 <= w3 <= 1:\n",
        "                weights = np.array([w1, w2, w3])\n",
        "                \n",
        "                # Calculate weighted prediction\n",
        "                weighted_pred = np.zeros_like(all_val_preds[0])\n",
        "                for i, pred in enumerate(all_val_preds):\n",
        "                    weighted_pred += weights[i] * pred\n",
        "                \n",
        "                # Calculate MSE\n",
        "                mse = np.mean((weighted_pred - val_targets) ** 2)\n",
        "                \n",
        "                if mse < best_mse:\n",
        "                    best_mse = mse\n",
        "                    best_weights = weights\n",
        "    \n",
        "    print(f\"Best weights found: {best_weights} with MSE: {best_mse:.6f}\")\n",
        "    return best_weights\n",
        "\n",
        "# Get optimal weights\n",
        "best_weights = optimize_ensemble_weights([lstm_model, gru_model, transformer_model], val_loader, device)\n",
        "\n",
        "# Create ensemble model\n",
        "ensemble_prediction_model = EnsemblePredictionModel(\n",
        "    models=[lstm_model, gru_model, transformer_model],\n",
        "    weights=best_weights\n",
        ")\n",
        "\n",
        "# Evaluate ensemble model\n",
        "def evaluate_ensemble_model(ensemble_model, data_loader, device):\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "    \n",
        "    for batch_x, batch_y in data_loader:\n",
        "        batch_x_np = batch_x.numpy()\n",
        "        batch_y_np = batch_y.numpy()\n",
        "        \n",
        "        outputs = ensemble_model.predict(batch_x_np, device)\n",
        "        \n",
        "        all_predictions.append(outputs)\n",
        "        all_targets.append(batch_y_np)\n",
        "    \n",
        "    all_predictions = np.vstack(all_predictions)\n",
        "    all_targets = np.vstack(all_targets)\n",
        "    \n",
        "    mse = np.mean((all_predictions - all_targets) ** 2)\n",
        "    \n",
        "    return mse, all_predictions, all_targets\n",
        "\n",
        "# Evaluate ensemble model\n",
        "ensemble_loss, ensemble_preds, ensemble_targets = evaluate_ensemble_model(\n",
        "    ensemble_prediction_model, test_loader, device\n",
        ")\n",
        "\n",
        "print(f\"\\nEnsemble Test MSE: {ensemble_loss:.6f}\")\n",
        "\n",
        "# Visualize ensemble prediction\n",
        "plot_prediction_sample(ensemble_preds, ensemble_targets, 'Ensemble', sample_idx)\n",
        "plt.savefig('ensemble_prediction_sample.png', dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# Compare all models\n",
        "print(\"\\n=== Model Comparison ===\")\n",
        "print(f\"{'Model':<12} {'Test MSE':<12}\")\n",
        "print(\"-\" * 25)\n",
        "print(f\"{'LSTM':<12} {lstm_loss:<12.6f}\")\n",
        "print(f\"{'GRU':<12} {gru_loss:<12.6f}\")\n",
        "print(f\"{'Transformer':<12} {transformer_loss:<12.6f}\")\n",
        "print(f\"{'Ensemble':<12} {ensemble_loss:<12.6f}\")\n",
        "\n",
        "# Plot comparison of all models for a sample\n",
        "def plot_all_models_comparison(predictions_dict, targets, sample_idx=0):\n",
        "    plt.figure(figsize=(14, 7))\n",
        "    \n",
        "    target = targets[sample_idx].squeeze()\n",
        "    plt.plot(target, 'k-', lw=2, label='Actual')\n",
        "    \n",
        "    for model_name, preds in predictions_dict.items():\n",
        "        pred = preds[sample_idx].squeeze()\n",
        "        plt.plot(pred, '--', label=f'{model_name}')\n",
        "    \n",
        "    plt.title('Model Comparison - Prediction Sample')\n",
        "    plt.xlabel('Time Step')\n",
        "    plt.ylabel('Normalized Value')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    \n",
        "    return plt.gcf()\n",
        "\n",
        "# Compare all models on a sample\n",
        "predictions_dict = {\n",
        "    'LSTM': lstm_preds,\n",
        "    'GRU': gru_preds,\n",
        "    'Transformer': transformer_preds,\n",
        "    'Ensemble': ensemble_preds\n",
        "}\n",
        "\n",
        "plot_all_models_comparison(predictions_dict, ensemble_targets, sample_idx)\n",
        "plt.savefig('model_comparison.png', dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# Calculate prediction errors\n",
        "def calculate_prediction_errors(predictions, targets):\n",
        "    return np.mean(np.abs(predictions - targets), axis=2).squeeze()\n",
        "\n",
        "lstm_errors = calculate_prediction_errors(lstm_preds, lstm_targets)\n",
        "gru_errors = calculate_prediction_errors(gru_preds, gru_targets)\n",
        "transformer_errors = calculate_prediction_errors(transformer_preds, transformer_targets)\n",
        "ensemble_errors = calculate_prediction_errors(ensemble_preds, ensemble_targets)\n",
        "\n",
        "# Plot error distribution\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.hist(lstm_errors, bins=50, alpha=0.5, label='LSTM')\n",
        "plt.hist(gru_errors, bins=50, alpha=0.5, label='GRU')\n",
        "plt.hist(transformer_errors, bins=50, alpha=0.5, label='Transformer')\n",
        "plt.hist(ensemble_errors, bins=50, alpha=0.5, label='Ensemble')\n",
        "\n",
        "plt.title('Error Distribution by Model')\n",
        "plt.xlabel('Mean Absolute Error')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.savefig('error_distribution.png', dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# Save models\n",
        "def save_prediction_models(models_dict, weights=None, base_path=\"models/prediction/\"):\n",
        "    import os\n",
        "    import json\n",
        "    \n",
        "    os.makedirs(base_path, exist_ok=True)\n",
        "    \n",
        "    for name, model in models_dict.items():\n",
        "        model_path = os.path.join(base_path, f\"{name}_model.pt\")\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "        logger.info(f\"Modelo {name} guardado en {model_path}\")\n",
        "    \n",
        "    if weights is not None:\n",
        "        # Save ensemble weights\n",
        "        weights_dict = {\n",
        "            \"lstm\": float(weights[0]),\n",
        "            \"gru\": float(weights[1]),\n",
        "            \"transformer\": float(weights[2])\n",
        "        }\n",
        "        \n",
        "        with open(os.path.join(base_path, \"ensemble_weights.json\"), \"w\") as f:\n",
        "            json.dump(weights_dict, f)\n",
        "        \n",
        "        logger.info(f\"Ensemble weights saved to {os.path.join(base_path, 'ensemble_weights.json')}\")\n",
        "    \n",
        "    # Save configuration\n",
        "    config = {\n",
        "        \"sequence_length\": sequence_length,\n",
        "        \"prediction_horizon\": prediction_horizon,\n",
        "        \"embedding_dim\": 64,\n",
        "        \"input_size\": 1\n",
        "    }\n",
        "    \n",
        "    with open(os.path.join(base_path, \"model_config.json\"), \"w\") as f:\n",
        "        json.dump(config, f)\n",
        "    \n",
        "    logger.info(f\"Configuration saved to {os.path.join(base_path, 'model_config.json')}\")\n",
        "\n",
        "# Save prediction models\n",
        "save_prediction_models(\n",
        "    models_dict={\n",
        "        \"lstm\": lstm_model,\n",
        "        \"gru\": gru_model,\n",
        "        \"transformer\": transformer_model\n",
        "    },\n",
        "    weights=best_weights,\n",
        "    base_path=\"models/prediction/\"\n",
        ")\n",
        "\n",
        "# Function to load prediction models\n",
        "def load_prediction_models(base_path=\"models/prediction/\"):\n",
        "    import os\n",
        "    import json\n",
        "    \n",
        "    # Load configuration\n",
        "    with open(os.path.join(base_path, \"model_config.json\"), \"r\") as f:\n",
        "        config = json.load(f)\n",
        "    \n",
        "    sequence_length = config[\"sequence_length\"]\n",
        "    prediction_horizon = config[\"prediction_horizon\"]\n",
        "    embedding_dim = config[\"embedding_dim\"]\n",
        "    input_size = config[\"input_size\"]\n",
        "    \n",
        "    # Initialize models\n",
        "    lstm_model = LSTMMultiStepPredictor(\n",
        "        input_size=input_size,\n",
        "        hidden_size=embedding_dim,\n",
        "        num_layers=2,\n",
        "        output_size=prediction_horizon,\n",
        "        dropout=0.2\n",
        "    )\n",
        "    \n",
        "    gru_model = GRUMultiStepPredictor(\n",
        "        input_size=input_size,\n",
        "        hidden_size=embedding_dim,\n",
        "        num_layers=2,\n",
        "        output_size=prediction_horizon,\n",
        "        dropout=0.2\n",
        "    )\n",
        "    \n",
        "    transformer_model = TransformerPredictor(\n",
        "        input_size=input_size,\n",
        "        d_model=embedding_dim,\n",
        "        nhead=4,\n",
        "        dim_feedforward=128,\n",
        "        num_layers=2,\n",
        "        output_size=prediction_horizon,\n",
        "        dropout=0.2\n",
        "    )\n",
        "    \n",
        "    # Load model weights\n",
        "    lstm_model.load_state_dict(torch.load(os.path.join(base_path, \"lstm_model.pt\")))\n",
        "    gru_model.load_state_dict(torch.load(os.path.join(base_path, \"gru_model.pt\")))\n",
        "    transformer_model.load_state_dict(torch.load(os.path.join(base_path, \"transformer_model.pt\")))\n",
        "    \n",
        "    # Load ensemble weights\n",
        "    with open(os.path.join(base_path, \"ensemble_weights.json\"), \"r\") as f:\n",
        "        weights_dict = json.load(f)\n",
        "    \n",
        "    weights = [weights_dict[\"lstm\"], weights_dict[\"gru\"], weights_dict[\"transformer\"]]\n",
        "    \n",
        "    # Create ensemble model\n",
        "    ensemble_model = EnsemblePredictionModel(\n",
        "        models=[lstm_model, gru_model, transformer_model],\n",
        "        weights=weights\n",
        "    )\n",
        "    \n",
        "    logger.info(f\"Prediction models loaded from {base_path}\")\n",
        "    \n",
        "    return {\n",
        "        \"lstm\": lstm_model,\n",
        "        \"gru\": gru_model,\n",
        "        \"transformer\": transformer_model,\n",
        "        \"ensemble\": ensemble_model,\n",
        "        \"config\": config\n",
        "    }\n",
        "\n",
        "# Function to make predictions with the saved models\n",
        "def make_predictions(new_data, models, scaler=None, device='cpu'):\n",
        "    \"\"\"Make predictions on new data using saved models.\"\"\"\n",
        "    # Preprocess data\n",
        "    if scaler is not None and not isinstance(new_data, np.ndarray):\n",
        "        new_data = scaler.transform(new_data.values.reshape(-1, 1))\n",
        "    \n",
        "    # Create sequences\n",
        "    config = models[\"config\"]\n",
        "    sequence_length = config[\"sequence_length\"]\n",
        "    \n",
        "    # Create sequence from most recent data\n",
        "    if len(new_data) >= sequence_length:\n",
        "        recent_data = new_data[-sequence_length:].reshape(1, sequence_length, 1)\n",
        "    else:\n",
        "        # If not enough data, pad with zeros\n",
        "        pad_size = sequence_length - len(new_data)\n",
        "        padding = np.zeros((pad_size, 1))\n",
        "        recent_data = np.concatenate([padding, new_data]).reshape(1, sequence_length, 1)\n",
        "    \n",
        "    # Make predictions\n",
        "    ensemble_model = models[\"ensemble\"]\n",
        "    predictions = ensemble_model.predict(recent_data, device)\n",
        "    \n",
        "    # Inverse transform if scaler provided\n",
        "    if scaler is not None:\n",
        "        predictions = scaler.inverse_transform(predictions.squeeze()).reshape(-1, 1)\n",
        "    \n",
        "    return predictions"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
